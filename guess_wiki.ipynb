{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdad99e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "658f7cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word\n",
       "nba             52\n",
       "league          50\n",
       "season          27\n",
       "team            19\n",
       "basketball      19\n",
       "championship    18\n",
       "player          15\n",
       "final           15\n",
       "first           14\n",
       "lakers          14\n",
       "franchise       13\n",
       "new             13\n",
       "celtic          12\n",
       "spur            10\n",
       "led             10\n",
       "title           10\n",
       "warrior         10\n",
       "game            10\n",
       "four            10\n",
       "baa             10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://levelup.gitconnected.com/two-simple-ways-to-scrape-text-from-wikipedia-in-python-9ce07426579b\n",
    "\n",
    "my_url = 'https://en.wikipedia.org/wiki/NBA'\n",
    "r = requests.get(my_url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# what elements are in soup \n",
    "#print(set([text.parent.name for text in soup.find_all(text=True)]))\n",
    "\n",
    "# Extract the plain text content from paragraphs\n",
    "paras = []\n",
    "for paragraph in soup.find_all('p'):\n",
    "    paras.append(str(paragraph.text))\n",
    "\n",
    "# Extract text from paragraph headers\n",
    "heads = []\n",
    "for head in soup.find_all('span', attrs={'mw-headline'}):\n",
    "    heads.append(str(head.text))\n",
    "\n",
    "# Interleave paragraphs & headers\n",
    "my_text = [val for pair in zip(paras, heads) for val in pair]\n",
    "my_text = ' '.join(my_text)\n",
    "\n",
    "# Drop footnote superscripts in brackets\n",
    "my_text = re.sub(r\"\\[.*?\\]+\", '', my_text)\n",
    "\n",
    "# Replace '\\n' (a new line) with '' and end the string at $1000.\n",
    "my_text = my_text.replace('\\n', '')[:-11]\n",
    "\n",
    "# NLTK\n",
    "bw = nltk.word_tokenize(my_text)\n",
    "\n",
    "def decontracted(text):\n",
    "    '''convert contractions with apostrophs into words'''\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", text)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", text)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", text)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", text)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "stop = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "#remove stop words, punctuation, numbers, uppercase\n",
    "bw = [decontracted(i) for i in word_tokenize(my_text.lower()) if not i in stop and i.isalpha()]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "bw = [lemmatizer.lemmatize(i) for i in bw] # lemmatization for nouns\n",
    "bw = [lemmatizer.lemmatize(i, pos='a') for i in bw] # lemmatization for adjectives\n",
    "\n",
    "df = pd.DataFrame(bw, columns=['word']).groupby('word').size().sort_values(ascending=False)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fef14b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
